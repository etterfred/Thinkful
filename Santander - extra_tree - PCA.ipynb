{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Fred Etter - March 2019  \n",
    "\n",
    "This notebook uses data from a Kaggle competition to predict if a customer will buy a product from Banco Santander during a typical transaction.  There are a total of 200,000 rows of customer transactions along with the binary outcome of whether they made a purchase or not.  There are 200 columns, or 200 features, to use as inputs to each of the models that are presented.  \n",
    "\n",
    "The workflow in this notebook is as follows:\n",
    "1.  Import and Clean Data  \n",
    "2.  Data Exploration  \n",
    "3.  Modeling and Evaluation\n",
    "4.  Conclusion and Discussion  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Import and clean the data  \n",
    "In this section, the data is imported and cleaned.  \n",
    "\n",
    "Import the data from a csv file; then print the number of rows and columns of the data; then show the first 5 lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 202)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Fred\\Documents\\PythonDirectory\\Unit 3\\train_san.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 200,000 rows and 202 columns.  All of the feature data are float numbers.  We can drop the ID_code later because it is irrevelant.  The var_1, var_2, etc are the column names given in the orginal dataset.  These features have not been specifically defined in Kaggle.  They are anonymized features that have been captured during customer transactions.  \n",
    "\n",
    "Next, we'll print the sum of the 'target' column which is the number of all of the rows where the customer bought a product.  Then we calculate the buy ratio.  As you can see here, a customer bought something about 10% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20098"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers average buy ratio, 0.10049\n"
     ]
    }
   ],
   "source": [
    "# print the sume of the target column\n",
    "df['target'].sum()\n",
    "\n",
    "# divide the target sum by the total number of rows\n",
    "print(\"Customers average buy ratio, {}\".format(df['target'].sum()/len(df['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know we have a highly imbalanced dataset; 10% are 'buy' transactions, 90% are 'not buy' transactions.  In order to capture this imbalance we will use following technique:\n",
    "- we will randomly remove most of the 'not buy' rows so the number of 'buy' and 'not buy' rows are even.  \n",
    "\n",
    "Next, let's look at some of the data types in our dataset and check for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_code     object\n",
       "target       int64\n",
       "var_0      float64\n",
       "var_1      float64\n",
       "var_2      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 'target' data type is an integer which is what we'll need for the machine learning algorithms.  We have float values for the columns - which is what we want - and an 'ID_code' that is an object (we'll delete this column later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, no cleaning necessary for NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the ID_code column since it is a string and irrelevant to making a buy or no_buy prediction.  Also, drop a random sample of 140,000 rows to account for hardware / memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ID_code column\n",
    "df.drop(['ID_code'], axis=1, inplace=True)\n",
    "\n",
    "# drop a random sample of 140,000 rows\n",
    "df.drop(df.sample(195000).index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the number of rows of the data to make sure we dropped correct amount and then shuffle the data randomly to prepare data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look at the data to see random shuffled rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127670</th>\n",
       "      <td>0</td>\n",
       "      <td>13.1134</td>\n",
       "      <td>0.9263</td>\n",
       "      <td>8.4017</td>\n",
       "      <td>7.0261</td>\n",
       "      <td>10.2461</td>\n",
       "      <td>-13.9023</td>\n",
       "      <td>6.0891</td>\n",
       "      <td>12.5448</td>\n",
       "      <td>-4.6748</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9387</td>\n",
       "      <td>6.0567</td>\n",
       "      <td>3.5393</td>\n",
       "      <td>4.7091</td>\n",
       "      <td>17.2364</td>\n",
       "      <td>-0.1680</td>\n",
       "      <td>-3.0132</td>\n",
       "      <td>10.3269</td>\n",
       "      <td>18.9674</td>\n",
       "      <td>-10.7604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91738</th>\n",
       "      <td>0</td>\n",
       "      <td>6.3999</td>\n",
       "      <td>3.5680</td>\n",
       "      <td>14.7459</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>10.9572</td>\n",
       "      <td>-5.4206</td>\n",
       "      <td>5.6457</td>\n",
       "      <td>23.9838</td>\n",
       "      <td>-3.2039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8688</td>\n",
       "      <td>4.3747</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>-7.0703</td>\n",
       "      <td>22.0905</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>5.6649</td>\n",
       "      <td>9.4528</td>\n",
       "      <td>16.8323</td>\n",
       "      <td>14.4631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138164</th>\n",
       "      <td>0</td>\n",
       "      <td>7.3829</td>\n",
       "      <td>-3.5773</td>\n",
       "      <td>11.7715</td>\n",
       "      <td>3.8721</td>\n",
       "      <td>12.6857</td>\n",
       "      <td>-9.6839</td>\n",
       "      <td>6.0539</td>\n",
       "      <td>15.8856</td>\n",
       "      <td>2.3073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5405</td>\n",
       "      <td>9.4594</td>\n",
       "      <td>1.7664</td>\n",
       "      <td>2.4748</td>\n",
       "      <td>16.4584</td>\n",
       "      <td>-1.3579</td>\n",
       "      <td>-7.5438</td>\n",
       "      <td>9.1225</td>\n",
       "      <td>19.7478</td>\n",
       "      <td>-5.1241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32634</th>\n",
       "      <td>0</td>\n",
       "      <td>18.9618</td>\n",
       "      <td>-10.0936</td>\n",
       "      <td>6.6822</td>\n",
       "      <td>6.2581</td>\n",
       "      <td>9.4195</td>\n",
       "      <td>-21.2975</td>\n",
       "      <td>6.3479</td>\n",
       "      <td>13.3900</td>\n",
       "      <td>-5.1408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>10.7950</td>\n",
       "      <td>1.8621</td>\n",
       "      <td>1.4947</td>\n",
       "      <td>17.5174</td>\n",
       "      <td>-1.9650</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>8.6497</td>\n",
       "      <td>14.5091</td>\n",
       "      <td>-14.1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5445</th>\n",
       "      <td>0</td>\n",
       "      <td>6.6791</td>\n",
       "      <td>-7.9880</td>\n",
       "      <td>9.2067</td>\n",
       "      <td>5.3087</td>\n",
       "      <td>12.5855</td>\n",
       "      <td>-3.8427</td>\n",
       "      <td>4.7047</td>\n",
       "      <td>14.6442</td>\n",
       "      <td>-1.9235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>13.9635</td>\n",
       "      <td>3.3346</td>\n",
       "      <td>-7.4988</td>\n",
       "      <td>14.7375</td>\n",
       "      <td>-0.7395</td>\n",
       "      <td>-6.7301</td>\n",
       "      <td>8.8540</td>\n",
       "      <td>13.6309</td>\n",
       "      <td>-9.5104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target    var_0    var_1    var_2   var_3    var_4    var_5   var_6  \\\n",
       "127670       0  13.1134   0.9263   8.4017  7.0261  10.2461 -13.9023  6.0891   \n",
       "91738        0   6.3999   3.5680  14.7459  4.7192  10.9572  -5.4206  5.6457   \n",
       "138164       0   7.3829  -3.5773  11.7715  3.8721  12.6857  -9.6839  6.0539   \n",
       "32634        0  18.9618 -10.0936   6.6822  6.2581   9.4195 -21.2975  6.3479   \n",
       "5445         0   6.6791  -7.9880   9.2067  5.3087  12.5855  -3.8427  4.7047   \n",
       "\n",
       "          var_7   var_8   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "127670  12.5448 -4.6748   ...      6.9387   6.0567   3.5393   4.7091  17.2364   \n",
       "91738   23.9838 -3.2039   ...      2.8688   4.3747   0.6455  -7.0703  22.0905   \n",
       "138164  15.8856  2.3073   ...      0.5405   9.4594   1.7664   2.4748  16.4584   \n",
       "32634   13.3900 -5.1408   ...      0.5705  10.7950   1.8621   1.4947  17.5174   \n",
       "5445    14.6442 -1.9235   ...      0.2276  13.9635   3.3346  -7.4988  14.7375   \n",
       "\n",
       "        var_195  var_196  var_197  var_198  var_199  \n",
       "127670  -0.1680  -3.0132  10.3269  18.9674 -10.7604  \n",
       "91738    0.0519   5.6649   9.4528  16.8323  14.4631  \n",
       "138164  -1.3579  -7.5438   9.1225  19.7478  -5.1241  \n",
       "32634   -1.9650   0.8396   8.6497  14.5091 -14.1486  \n",
       "5445    -0.7395  -6.7301   8.8540  13.6309  -9.5104  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our data looks clean and ready to start exploring in more detail to look for features that might be better predictors for the target data.  We can use the pairplot and the heatmap functionality to begin this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Data Exploration  \n",
    "First, let's look at the correlation between the target and the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target   var_169    0.082870\n",
       "         var_109    0.076477\n",
       "         var_33     0.074832\n",
       "var_34   target     0.068878\n",
       "var_149  target     0.068421\n",
       "var_146  target     0.066935\n",
       "target   var_139    0.065506\n",
       "var_166  target     0.065490\n",
       "target   var_122    0.064091\n",
       "         var_13     0.062257\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use absolute value\n",
    "np.absolute(df.corr().unstack().sort_values().drop_duplicates())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any strongly correlated features in this dataset.  The highest correlation is only around 9%.\n",
    "\n",
    "\n",
    "Next, we'll look at the pairplot for the top 10 features as determined by the correlations found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pp = df[['target', 'var_81', 'var_139', 'var_12', 'var_174', \n",
    "#             'var_146', 'var_80', 'var_76', 'var_165', 'var_148', 'var_166']].copy()\n",
    "\n",
    "# sns.set(font_scale=1.7)\n",
    "# sns.pairplot(df_pp)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too exciting here; looks like a lot of noise.  Features look highly uncorrelated.  We do see that all of these features are very close to normally distributed.  We can also see the previously observed ratio of 1 buy transaction for every 10 transations.  \n",
    "\n",
    "We can look at a heatmap for the 10 best features (determined above) for a more visual representation of the correlations between target and feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(10,9))\n",
    "# corr = df_pp.corr()\n",
    "# sns.heatmap(corr, cmap='coolwarm')\n",
    "# plt.savefig('heatmat.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heatmap and correlation values show that there is very little correlation between the target variable and the top 10 best predictive features.  It looks like, at this point, we will need to incorporate many features (maybe most of the 200) to improve the predictive value of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Build the models and evaluate\n",
    "The first step to building a good model is to separate the data into training and test data.  We'll train the model on the training data and test it with the test data.  This next line of code breaks the dataframe into 2 dataframes: 1 for training and 1 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets.\n",
    "offset = int(df.shape[0] * 0.8)\n",
    "\n",
    "df_train = df[:offset]\n",
    "df_test = df[offset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new shapes of the 2 new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 201)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 201)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, confirm the existence of about 10% 'buy' rows for the training data. (8034 is about 10% of 80,000 from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers average buy ratio, 0.101\n"
     ]
    }
   ],
   "source": [
    "print(\"Customers average buy ratio, {0:.3f}\".format(df['target'].sum()/len(df['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to balance the data to account for the current 9 to 1 ratio of 'not buy' to 'buy'.  In this case, dropping around 90% of the zeros was performed to ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "(832, 201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fred\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# set variable buy_num to the total number of buys\n",
    "buy_num = df_train['target'].sum()\n",
    "print(buy_num)\n",
    "\n",
    "# calculate the difference between the total rows and the number of buys\n",
    "diff = df_train.shape[0] - buy_num\n",
    "\n",
    "# calculate the number of rows to drop and drop those rows\n",
    "df_train.drop(df_train.query('target < 1').sample(frac=(1 - (buy_num/diff))).index, inplace=True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new shape of the training data.  The new training dataframe has an equal number of 'buy' and 'not buy' rows.  As you can see, the number of rows has twice the number of the number of 'buys' in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buys, 416\n",
      "Number of rows and columns, (832, 201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of buys, {}\".format(buy_num))\n",
    "print(\"Number of rows and columns, {}\".format(df_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Area Under the ROC curve insead of the more typical r-squared accuracy measure.  This is the preferred metric in accordance with Kaggle scoring.\n",
    "\n",
    "Basic Logistic Regression is the first model used to calculate the ROC accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Logistic Regression without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fred\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duration: 0:00:00.192011\n"
     ]
    }
   ],
   "source": [
    "# 1.  Logistic Regression without PCA\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Instantiate our model.\n",
    "regr = linear_model.LogisticRegression(solver='sag')\n",
    "\n",
    "# set features and dependent variable for training data\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "# drop the 'target' column to obtain the feature inputs\n",
    "df_train.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "# normalize the training data\n",
    "x_train = sklearn.preprocessing.normalize(df_train)\n",
    "\n",
    "# now for test...\n",
    "y_test = df_test['target'].values\n",
    "\n",
    "# drop the 'target' column to obtain the feature inputs\n",
    "df_test.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "# normalize the test data\n",
    "x_test = sklearn.preprocessing.normalize(df_test)\n",
    "\n",
    "# fit model to training data\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the are under the ROC curve next for the Linear Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6668661371631669"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_test_pred = regr.predict(x_test)\n",
    "sklearn.metrics.roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Confusion Matrix for the LR Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[633, 276],\n",
       "       [ 33,  58]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "sklearn.metrics.confusion_matrix(y_test, y_test_pred, labels=None, sample_weight=None)\n",
    "# print(len(y_test), np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the raw accuracy for the LR Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2  Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "\n",
      "Duration: 0:00:00.177010\n"
     ]
    }
   ],
   "source": [
    "# 1.  Logistic Regression\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Instantiate our model.\n",
    "regr_pca = linear_model.LogisticRegression(solver='sag')\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "# .95 means sklearn will chose the minimum number of components such that 95% of the variance is retained.\n",
    "pca = PCA(.95) \n",
    "\n",
    "pca.fit(x_train)\n",
    "\n",
    "print(pca.n_components_)\n",
    "\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# fit model to training data\n",
    "regr_pca.fit(x_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6641098175751641"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_test_pred = regr_pca.predict(x_test)\n",
    "sklearn.metrics.roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[618, 291],\n",
       "       [ 32,  59]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "sklearn.metrics.confusion_matrix(y_test, y_test_pred, labels=None, sample_weight=None)\n",
    "# print(len(y_test), np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.677"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "LR without PCA:\n",
    "- area under ROC:  0.667\n",
    "- raw accuracy:  0.691  \n",
    "\n",
    "LR with PCA:  \n",
    "- area under ROC:  0.664  \n",
    "- raw accuracy:  0.677  \n",
    "\n",
    "Number of features retained was 106 out of 200."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
